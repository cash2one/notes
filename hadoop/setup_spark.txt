http://book.51cto.com/art/201408/448416.htm

master的web UI(默认 http://localhost:8080  


前提条件
1 hadoop已经搭建完成
2 master, slave01, slave02     
                                                                                |
1 安装Scala                                                                          |
2 安装spark                                                                          | 
  下载后"spark-1.0.2-bin-hadoop2.tgz"解压到"/usr/local/spark"目录之下.配置"~/.bashrc"|
  , 设置"SPARK_HOME"并把Spark的bin目录加入到PATH之中, 配置完成后使用source命令使配置 |
  生效.                                                                              |
3 修改配置文件                                                                       |
3.1 conf/slaves 
  slave01
  slave02                                                                            |
3.2 conf/spark-env.sh                                                                |
  首先把spark-env.sh.template拷贝到spark-env.sh. 设置一些变量, 如                    |
  export SCALA_HOME=/usr/lib/scala-2.9.3                                             |         
4 在所有worker上安装并配置Spark                                                      |
  既然master上的这个文件件已经配置好了, 把它拷贝到所有的worker. 注意, 三台机器spark所|
  在目录必须一致, 因为master会登陆到worker上执行命令, master认为worker的spark路径与自|
  己一样.
  $ cd
  $ scp -r spark-0.7.2 dev@slave01:~
  $ scp -r spark-0.7.2 dev@slave02:~                                                 |
5 启动Spark集群                                                                      |
  在master上执行
  $ cd ~/spark-0.7.2
  $ bin/start-all.sh
  检测进程是否启动
  $ jps
  11055 Jps
  2313 SecondaryNameNode
  2409 JobTracker
  2152 NameNode
  4822 Master                                                                        |
  浏览master的web UI(默认http://localhost:8080). 这是你应该可以看到所有的word节点, 以|
  及他们的CPU个数和内存等信息
5.6 运行SparkPi例子

$ cd ~/spark-0.7.2
$ ./run spark.examples.SparkPi spark://master:7077
（可选）运行自带的例子，SparkLR 和 SparkKMeans.

#Logistic Regression
#./run spark.examples.SparkLR spark://master:7077
#kmeans
$ ./run spark.examples.SparkKMeans spark://master:7077 ./kmeans_data.txt 2 1
5.7 从HDFS读取文件并运行WordCount
$ cd ~/spark-0.7.2
$ hadoop fs -put README.md .
$ MASTER=spark://master:7077 ./spark-shell
scala> val file = sc.textFile("hdfs://master:9000/user/dev/README.md")
scala> val count = file.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_)
scala> count.collect()
5.8 停止 Spark 集群
$ cd ~/spark-0.7.2
$ bin/stop-all.sh  